# https://mypy.readthedocs.io/en/stable/common_issues.html#using-classes-that-are-generic-in-stubs-but-not-at-runtime
from __future__ import annotations

import binascii
import stat as stat_module
import collections
import concurrent.futures
import contextlib
import hashlib
import io
import itertools
import math
import multiprocessing as mp
import os
import re
import shutil
import stat as stat_module
import tempfile
import time
import urllib.parse
from types import ModuleType
from typing import (
    TYPE_CHECKING,
    Any,
    BinaryIO,
    Callable,
    Iterator,
    List,
    NamedTuple,
    Optional,
    Sequence,
    TextIO,
    Tuple,
    Union,
    cast,
    overload,
)

import urllib3

if TYPE_CHECKING:
    # Literal is only in the stdlib in Python 3.8+
    # this works without having a runtime installation of typing_extensions because
    # a) we postponed evaluation of type annotations with PEP 563,
    # b) we don't use Literal as a base class or for casting,
    # c) type checkers always know what typing_extensions is
    from typing_extensions import Literal

import filelock

from blobfile import _azure as azure
from blobfile import _common as common
from blobfile import _gcp as gcp
from blobfile._common import (
    CHUNK_SIZE,
    DEFAULT_CONNECTION_POOL_MAX_SIZE,
    DEFAULT_MAX_CONNECTION_POOL_COUNT,
    Config,
    DirEntry,
    Error,
    Request,
    RestartableStreamingWriteFailure,
    Stat,
    get_log_threshold_for_error,
)

# https://cloud.google.com/storage/docs/naming
# https://www.w3.org/TR/xml/#charsets
INVALID_CHARS = (
    set().union(range(0x0, 0x9)).union(range(0xB, 0xE)).union(range(0xE, 0x20))
)

DEFAULT_AZURE_WRITE_CHUNK_SIZE = 8 * 2 ** 20
DEFAULT_GOOGLE_WRITE_CHUNK_SIZE = 8 * 2 ** 20
DEFAULT_RETRY_LOG_THRESHOLD = 0
DEFAULT_RETRY_COMMON_LOG_THRESHOLD = 2
DEFAULT_CONNECT_TIMEOUT = 10
DEFAULT_READ_TIMEOUT = 30
DEFAULT_BUFFER_SIZE = 8 * 2 ** 20


def _execute_fn_and_ignore_result(fn: Callable, *args: Any):
    fn(*args)


class Context:
    def __init__(self, conf: Config):
        self._conf = conf

    def copy(
        self,
        src: str,
        dst: str,
        overwrite: bool = False,
        parallel: bool = False,
        parallel_executor: Optional[concurrent.futures.Executor] = None,
        return_md5: bool = False,
    ) -> Optional[str]:
        # it would be best to check isdir() for remote paths, but that would
        # involve 2 extra network requests, so just do this test instead
        if _guess_isdir(src):
            raise IsADirectoryError(f"Is a directory: '{src}'")
        if _guess_isdir(dst):
            raise IsADirectoryError(f"Is a directory: '{dst}'")

        if not overwrite:
            if self.exists(dst):
                raise FileExistsError(
                    f"Destination '{dst}' already exists and overwrite is disabled"
                )

        if parallel:
            copy_fn = None
            if (_is_azure_path(src) or _is_gcp_path(src)) and _is_local_path(dst):
                copy_fn = _parallel_download

            if _is_local_path(src) and _is_azure_path(dst):
                copy_fn = azure.parallel_upload

            if _is_local_path(src) and _is_gcp_path(dst):
                copy_fn = gcp.parallel_upload

            if _is_azure_path(src) and _is_azure_path(dst):
                src_account, _, _ = azure.split_path(src)
                dst_account, _, _ = azure.split_path(dst)
                if src_account != dst_account:
                    # normal remote copy is pretty fast and doesn't benefit from parallelization when used within
                    # a storage account
                    copy_fn = azure.parallel_remote_copy

            if copy_fn is not None:
                if parallel_executor is None:
                    with concurrent.futures.ProcessPoolExecutor(
                        mp_context=mp.get_context(
                            self._conf.multiprocessing_start_method
                        )
                    ) as executor:
                        return copy_fn(
                            self._conf, executor, src, dst, return_md5=return_md5
                        )
                else:
                    return copy_fn(
                        self._conf, parallel_executor, src, dst, return_md5=return_md5
                    )

        # special case cloud to cloud copy, don't download the file
        if _is_gcp_path(src) and _is_gcp_path(dst):
            return gcp.remote_copy(self._conf, src=src, dst=dst, return_md5=return_md5)

        if _is_azure_path(src) and _is_azure_path(dst):
            return azure.remote_copy(
                self._conf, src=src, dst=dst, return_md5=return_md5
            )

        for attempt, backoff in enumerate(common.exponential_sleep_generator()):
            try:
                with self.BlobFile(src, "rb", streaming=True) as src_f, self.BlobFile(
                    dst, "wb", streaming=True
                ) as dst_f:
                    m = hashlib.md5()
                    while True:
                        block = src_f.read(CHUNK_SIZE)
                        if block == b"":
                            break
                        if return_md5:
                            m.update(block)
                        dst_f.write(block)  # type: ignore
                    if return_md5:
                        return m.hexdigest()
                    else:
                        return
            except RestartableStreamingWriteFailure as err:
                # currently this is the only type of failure we retry, since we can re-read the source
                # stream from the beginning
                # if this failure occurs, the upload must be restarted from the beginning
                # https://cloud.google.com/storage/docs/resumable-uploads#practices
                # https://github.com/googleapis/gcs-resumable-upload/issues/15#issuecomment-249324122
                if (
                    self._conf.retry_limit is not None
                    and attempt >= self._conf.retry_limit
                ):
                    raise

                if attempt >= get_log_threshold_for_error(self._conf, str(err)):
                    self._conf.log_callback(
                        f"error {err} when executing a streaming write to {dst} attempt {attempt}, sleeping for {backoff:.1f} seconds before retrying"
                    )
                time.sleep(backoff)

    def exists(self, path: str) -> bool:
        if _is_local_path(path):
            return os.path.exists(path)
        elif _is_gcp_path(path):
            st = gcp.maybe_stat(self._conf, path)
            if st is not None:
                return True
            return self.isdir(path)
        elif _is_azure_path(path):
            st = azure.maybe_stat(self._conf, path)
            if st is not None:
                return True
            return self.isdir(path)
        else:
            raise Error(f"Unrecognized path: '{path}'")

    def basename(self, path: str) -> str:
        if _is_gcp_path(path):
            _, obj = gcp.split_path(path)
            return obj.split("/")[-1]
        elif _is_azure_path(path):
            _, _, obj = azure.split_path(path)
            return obj.split("/")[-1]
        else:
            return os.path.basename(path)

    def glob(self, pattern: str, parallel: bool = False) -> Iterator[str]:
        if _is_local_path(pattern):
            # scanglob currently does an os.stat for each matched file
            # until scanglob can be implemented directly on scandir
            # this code is here to avoid that
            if "?" in pattern or "[" in pattern or "]" in pattern:
                raise Error("Advanced glob queries are not supported")
            yield from _local_glob(pattern)
        else:
            for entry in self.scanglob(pattern=pattern, parallel=parallel):
                yield entry.path

    def scanglob(
        self, pattern: str, parallel: bool = False, shard_prefix_length: int = 0
    ) -> Iterator[DirEntry]:
        if "?" in pattern or "[" in pattern or "]" in pattern:
            raise Error("Advanced glob queries are not supported")

        if _is_local_path(pattern):
            for filepath in _local_glob(pattern):
                # doing a stat call for each file isn't the most efficient
                # iglob uses os.scandir internally, but doesn't expose the information from that, so we'd
                # need to re-implement local glob
                # we could make the behavior with remote glob more consistent though if we did that
                s = os.stat(filepath)
                is_dir = stat_module.S_ISDIR(s.st_mode)
                yield DirEntry(
                    path=filepath,
                    name=self.basename(filepath),
                    is_dir=is_dir,
                    is_file=not is_dir,
                    stat=None
                    if is_dir
                    else Stat(
                        size=s.st_size,
                        mtime=s.st_mtime,
                        ctime=s.st_ctime,
                        md5=None,
                        version=None,
                    ),
                )
        elif _is_gcp_path(pattern) or _is_azure_path(pattern):
            if "*" not in pattern:
                entry = _get_entry(self._conf, pattern)
                if entry is not None:
                    yield entry
                return

            if _is_gcp_path(pattern):
                bucket, blob_prefix = gcp.split_path(pattern)
                if "*" in bucket:
                    raise Error("Wildcards cannot be used in bucket name")
                root = gcp.combine_path(bucket, "")
            else:
                account, container, blob_prefix = azure.split_path(pattern)
                if "*" in account or "*" in container:
                    raise Error("Wildcards cannot be used in account or container")
                root = azure.combine_path(self._conf, account, container, "")

            if shard_prefix_length == 0:
                initial_tasks = [_GlobTask("", _split_path(blob_prefix))]
            else:
                assert (
                    parallel
                ), "You probably want to use parallel=True if you are setting shard_prefix_length > 0"
                initial_tasks = []
                valid_chars = [
                    i
                    for i in range(256)
                    if i not in INVALID_CHARS.union([ord("/"), ord("*")])
                ]
                pattern_prefix, pattern_suffix = blob_prefix.split("*", maxsplit=1)
                for repeat in range(1, shard_prefix_length + 1):
                    for chars in itertools.product(valid_chars, repeat=repeat):
                        prefix = ""
                        for c in chars:
                            prefix += chr(c)
                        # we need to check for exact matches for shorter prefix lengths
                        # if we only searched for prefixes of length `shard_prefix_length`
                        # we would skip shorter names, for instance "a" would be skipped if we
                        # we had `shard_prefix_length=2`
                        # instead we check for an exact match for everything shorter than
                        # our `shard_prefix_length`
                        exact = repeat != shard_prefix_length
                        if exact:
                            pat = pattern_prefix + prefix
                        else:
                            pat = pattern_prefix + prefix + "*" + pattern_suffix
                        initial_tasks.append(_GlobTask("", _split_path(pat)))

            if parallel:
                mp_ctx = mp.get_context(self._conf.multiprocessing_start_method)
                tasks = mp_ctx.Queue()
                for t in initial_tasks:
                    tasks.put(t)
                tasks_enqueued = len(initial_tasks)
                results = mp_ctx.Queue()

                tasks_done = 0
                with mp_ctx.Pool(
                    initializer=_glob_worker,
                    initargs=(self._conf, root, tasks, results),
                ):
                    while tasks_done < tasks_enqueued:
                        r = results.get()
                        if isinstance(r, _GlobEntry):
                            yield r.entry
                        elif isinstance(r, _GlobTask):
                            tasks.put(r)
                            tasks_enqueued += 1
                        elif isinstance(r, _GlobTaskComplete):
                            tasks_done += 1
                        else:
                            raise Error("Invalid result")
            else:
                dq: collections.deque[_GlobTask] = collections.deque()
                for t in initial_tasks:
                    dq.append(t)
                while len(dq) > 0:
                    t = dq.popleft()
                    for r in _process_glob_task(conf=self._conf, root=root, t=t):
                        if isinstance(r, _GlobEntry):
                            yield r.entry
                        else:
                            dq.append(r)
        else:
            raise Error(f"Unrecognized path '{pattern}'")

    def isdir(self, path: str) -> bool:
        if _is_local_path(path):
            return os.path.isdir(path)
        elif _is_gcp_path(path):
            return gcp.isdir(self._conf, path)
        elif _is_azure_path(path):
            return azure.isdir(self._conf, path)
        else:
            raise Error(f"Unrecognized path: '{path}'")

    def listdir(self, path: str, shard_prefix_length: int = 0) -> Iterator[str]:
        for entry in self.scandir(path, shard_prefix_length=shard_prefix_length):
            yield entry.name

    def scandir(self, path: str, shard_prefix_length: int = 0) -> Iterator[DirEntry]:
        if (_is_gcp_path(path) or _is_azure_path(path)) and not path.endswith("/"):
            path += "/"
        if not self.exists(path):
            raise FileNotFoundError(
                f"The system cannot find the path specified: '{path}'"
            )
        if not self.isdir(path):
            raise NotADirectoryError(f"The directory name is invalid: '{path}'")
        if _is_local_path(path):
            for de in os.scandir(path):
                if de.is_dir():
                    yield DirEntry(
                        name=de.name,
                        path=os.path.abspath(de.path),
                        is_dir=True,
                        is_file=False,
                        stat=None,
                    )
                else:
                    s = de.stat()
                    yield DirEntry(
                        name=de.name,
                        path=os.path.abspath(de.path),
                        is_dir=False,
                        is_file=True,
                        stat=Stat(
                            size=s.st_size,
                            mtime=s.st_mtime,
                            ctime=s.st_ctime,
                            md5=None,
                            version=None,
                        ),
                    )
        elif _is_gcp_path(path) or _is_azure_path(path):
            if shard_prefix_length == 0:
                yield from _list_blobs_in_dir(
                    conf=self._conf, prefix=path, exclude_prefix=True
                )
            else:
                mp_ctx = mp.get_context(self._conf.multiprocessing_start_method)
                prefixes = mp_ctx.Queue()
                items = mp_ctx.Queue()
                tasks_enqueued = 0

                valid_chars = [
                    i for i in range(256) if i not in INVALID_CHARS and i != ord("/")
                ]
                for repeat in range(1, shard_prefix_length + 1):
                    for chars in itertools.product(valid_chars, repeat=repeat):
                        prefix = ""
                        for c in chars:
                            prefix += chr(c)
                        # we need to check for exact matches for shorter prefix lengths
                        # if we only searched for prefixes of length `shard_prefix_length`
                        # we would skip shorter names, for instance "a" would be skipped if we
                        # we had `shard_prefix_length=2`
                        # instead we check for an exact match for everything shorter than
                        # our `shard_prefix_length`
                        exact = repeat != shard_prefix_length
                        prefixes.put((path, prefix, exact))
                        tasks_enqueued += 1

                tasks_done = 0
                with mp_ctx.Pool(
                    initializer=_sharded_listdir_worker,
                    initargs=(self._conf, prefixes, items),
                ):
                    while tasks_done < tasks_enqueued:
                        entry = items.get()
                        if entry is None:
                            tasks_done += 1
                            continue
                        yield entry
        else:
            raise Error(f"Unrecognized path: '{path}'")

    def makedirs(self, path: str) -> None:
        if _is_local_path(path):
            os.makedirs(path, exist_ok=True)
        elif _is_gcp_path(path):
            gcp.mkdirfile(self._conf, path)
        elif _is_azure_path(path):
            azure.mkdirfile(self._conf, path)
        else:
            raise Error(f"Unrecognized path: '{path}'")

    def remove(self, path: str) -> None:
        if _is_local_path(path):
            os.remove(path)
        elif _is_gcp_path(path):
            if path.endswith("/"):
                raise IsADirectoryError(f"Is a directory: '{path}'")
            ok = gcp.remove(self._conf, path)
            if not ok:
                raise FileNotFoundError(
                    f"The system cannot find the path specified: '{path}'"
                )
        elif _is_azure_path(path):
            if path.endswith("/"):
                raise IsADirectoryError(f"Is a directory: '{path}'")
            ok = azure.remove(self._conf, path)
            if not ok:
                raise FileNotFoundError(
                    f"The system cannot find the path specified: '{path}'"
                )
        else:
            raise Error(f"Unrecognized path: '{path}'")

    def rmdir(self, path: str) -> None:
        if _is_local_path(path):
            os.rmdir(path)
            return

        # directories in blob storage are different from normal directories
        # a directory exists if there are any blobs that have that directory as a prefix
        # when the last blob with that prefix is deleted, the directory no longer exists
        # except in the case when there is a blob with a name ending in a slash
        # representing an empty directory

        # to make this more usable it is not an error to delete a directory that does
        # not exist, but is still an error to delete a non-empty one
        if not path.endswith("/"):
            path += "/"

        if _is_gcp_path(path):
            _, blob = gcp.split_path(path)
        elif _is_azure_path(path):
            _, _, blob = azure.split_path(path)
        else:
            raise Error(f"Unrecognized path: '{path}'")

        if blob == "":
            raise Error(f"Cannot delete bucket: '{path}'")
        it = self.listdir(path)
        try:
            next(it)
        except FileNotFoundError:
            # this directory does not exist
            return
        except StopIteration:
            # this directory exists and is empty
            pass
        else:
            # this directory exists but is not empty
            raise OSError(f"The directory is not empty: '{path}'")

        if _is_gcp_path(path):
            bucket, blob = gcp.split_path(path)
            req = Request(
                url=gcp.build_url(
                    "/storage/v1/b/{bucket}/o/{object}", bucket=bucket, object=blob
                ),
                method="DELETE",
                success_codes=(204,),
            )
            gcp.execute_api_request(self._conf, req)
        elif _is_azure_path(path):
            account, container, blob = azure.split_path(path)
            req = Request(
                url=azure.build_url(
                    account, "/{container}/{blob}", container=container, blob=blob
                ),
                method="DELETE",
                success_codes=(202,),
            )
            azure.execute_api_request(self._conf, req)
        else:
            raise Error(f"Unrecognized path: '{path}'")

    def stat(self, path: str) -> Stat:
        if _is_local_path(path):
            s = os.stat(path)
            return Stat(
                size=s.st_size,
                mtime=s.st_mtime,
                ctime=s.st_ctime,
                md5=None,
                version=None,
            )
        elif _is_gcp_path(path):
            st = gcp.maybe_stat(self._conf, path)
            if st is None:
                raise FileNotFoundError(f"No such file: '{path}'")
            return st
        elif _is_azure_path(path):
            st = azure.maybe_stat(self._conf, path)
            if st is None:
                raise FileNotFoundError(f"No such file: '{path}'")
            return st
        else:
            raise Error(f"Unrecognized path: '{path}'")

    def set_mtime(self, path: str, mtime: float, version: Optional[str] = None) -> bool:
        if _is_local_path(path):
            assert version is None
            os.utime(path, times=(mtime, mtime))
            return True
        elif _is_gcp_path(path):
            return gcp.set_mtime(self._conf, path=path, mtime=mtime, version=version)
        elif _is_azure_path(path):
            return azure.set_mtime(self._conf, path=path, mtime=mtime, version=version)
        else:
            raise Error(f"Unrecognized path: '{path}'")

    def rmtree(
        self,
        path: str,
        parallel: bool = False,
        parallel_executor: Optional[concurrent.futures.Executor] = None,
    ) -> None:
        if not self.isdir(path):
            raise NotADirectoryError(f"The directory name is invalid: '{path}'")

        if _is_local_path(path):
            shutil.rmtree(path)
        elif _is_gcp_path(path) or _is_azure_path(path):
            if not path.endswith("/"):
                path += "/"

            if _is_gcp_path(path):

                def request_generator():
                    bucket, blob = gcp.split_path(path)
                    for entry in gcp.list_blobs(self._conf, path):
                        entry_slash_path = _get_slash_path(entry)
                        entry_bucket, entry_blob = gcp.split_path(entry_slash_path)
                        assert entry_bucket == bucket and entry_blob.startswith(blob)
                        req = Request(
                            url=gcp.build_url(
                                "/storage/v1/b/{bucket}/o/{object}",
                                bucket=bucket,
                                object=entry_blob,
                            ),
                            method="DELETE",
                            # 404 is allowed in case a failed request successfully deleted the file
                            # before erroring out
                            success_codes=(204, 404),
                        )
                        yield req

                fn = gcp.execute_api_request

            elif _is_azure_path(path):

                def request_generator():
                    account, container, blob = azure.split_path(path)
                    for entry in azure.list_blobs(self._conf, path):
                        entry_slash_path = _get_slash_path(entry)
                        entry_account, entry_container, entry_blob = azure.split_path(
                            entry_slash_path
                        )
                        assert (
                            entry_account == account
                            and entry_container == container
                            and entry_blob.startswith(blob)
                        )
                        req = Request(
                            url=azure.build_url(
                                account,
                                "/{container}/{blob}",
                                container=container,
                                blob=entry_blob,
                            ),
                            method="DELETE",
                            # 404 is allowed in case a failed request successfully deleted the file
                            # before erroring out
                            success_codes=(202, 404),
                        )
                        yield req

                fn = azure.execute_api_request
            else:
                raise Error(f"Unrecognized path: '{path}'")

            if parallel:
                if parallel_executor is None:
                    executor = concurrent.futures.ProcessPoolExecutor(
                        mp_context=mp.get_context(
                            self._conf.multiprocessing_start_method
                        )
                    )
                    context = executor
                else:
                    executor = parallel_executor
                    context = contextlib.nullcontext()

                futures = []
                with context:
                    for req in request_generator():
                        f = executor.submit(
                            _execute_fn_and_ignore_result, fn, self._conf, req
                        )
                        futures.append(f)
                for f in futures:
                    f.result()
            else:
                for req in request_generator():
                    fn(self._conf, req)
        else:
            raise Error(f"Unrecognized path: '{path}'")

    def walk(
        self,
        top: str,
        topdown: bool = True,
        onerror: Optional[Callable[[OSError], None]] = None,
    ) -> Iterator[Tuple[str, Sequence[str], Sequence[str]]]:
        if not self.isdir(top):
            return

        if _is_local_path(top):
            top = os.path.normpath(top)
            for root, dirnames, filenames in os.walk(
                top=top, topdown=topdown, onerror=onerror
            ):
                assert isinstance(root, str)
                if root.endswith(os.sep):
                    root = root[:-1]
                yield (root, sorted(dirnames), sorted(filenames))
        elif _is_gcp_path(top) or _is_azure_path(top):
            top = _normalize_path(self._conf, top)
            if not top.endswith("/"):
                top += "/"
            if topdown:
                dq: collections.deque[str] = collections.deque()
                dq.append(top)
                while len(dq) > 0:
                    cur = dq.popleft()
                    assert cur.endswith("/")
                    if _is_gcp_path(top):
                        it = gcp.list_blobs(self._conf, cur, delimiter="/")
                    elif _is_azure_path(top):
                        it = azure.list_blobs(self._conf, cur, delimiter="/")
                    else:
                        raise Error(f"Unrecognized path: '{top}'")
                    dirnames = []
                    filenames = []
                    for entry in it:
                        entry_path = _get_slash_path(entry)
                        if entry_path == cur:
                            continue
                        if entry.is_dir:
                            dirnames.append(entry.name)
                        else:
                            filenames.append(entry.name)
                    yield (_strip_slash(cur), dirnames, filenames)
                    dq.extend(self.join(cur, dirname) + "/" for dirname in dirnames)
            else:
                if _is_gcp_path(top):
                    it = gcp.list_blobs(self._conf, top)
                elif _is_azure_path(top):
                    it = azure.list_blobs(self._conf, top)
                else:
                    raise Error(f"Unrecognized path: '{top}'")

                cur = []
                dirnames_stack = [[]]
                filenames_stack = [[]]
                for entry in it:
                    entry_slash_path = _get_slash_path(entry)
                    if entry_slash_path == top:
                        continue
                    relpath = entry_slash_path[len(top) :]
                    parts = relpath.split("/")
                    dirpath = parts[:-1]
                    if dirpath != cur:
                        # pop directories from the current path until we match the prefix of this new path
                        while cur != dirpath[: len(cur)]:
                            yield (
                                top + "/".join(cur),
                                dirnames_stack.pop(),
                                filenames_stack.pop(),
                            )
                            cur.pop()
                        # push directories from the new path until the current path matches it
                        while cur != dirpath:
                            dirname = dirpath[len(cur)]
                            cur.append(dirname)
                            filenames_stack.append([])
                            # add this to child dir to the list of dirs for the parent
                            dirnames_stack[-1].append(dirname)
                            dirnames_stack.append([])
                    if entry.is_file:
                        filenames_stack[-1].append(entry.name)
                while len(cur) > 0:
                    yield (
                        top + "/".join(cur),
                        dirnames_stack.pop(),
                        filenames_stack.pop(),
                    )
                    cur.pop()
                yield (_strip_slash(top), dirnames_stack.pop(), filenames_stack.pop())
                assert len(dirnames_stack) == 0 and len(filenames_stack) == 0
        else:
            raise Error(f"Unrecognized path: '{top}'")

    def dirname(self, path: str) -> str:
        if _is_gcp_path(path):
            return gcp.dirname(self._conf, path)
        elif _is_azure_path(path):
            return azure.dirname(self._conf, path)
        else:
            return os.path.dirname(path)

    def join(self, a: str, *args: str) -> str:
        out = a
        for b in args:
            out = _join2(self._conf, out, b)
        return out

    def get_url(self, path: str) -> Tuple[str, Optional[float]]:
        if _is_gcp_path(path):
            return gcp.get_url(self._conf, path)
        elif _is_azure_path(path):
            return azure.get_url(self._conf, path)
        elif _is_local_path(path):
            return f"file://{path}", None
        else:
            raise Error(f"Unrecognized path: '{path}'")

    def md5(self, path: str) -> str:
        if _is_gcp_path(path):
            st = gcp.maybe_stat(self._conf, path)
            if st is None:
                raise FileNotFoundError(f"No such file: '{path}'")

            h = st.md5
            if h is not None:
                return h

            # this is probably a composite object, calculate the md5 and store it on the file if the file has not changed
            with self.BlobFile(path, "rb") as f:
                result = common.block_md5(f).hex()

            assert st.version is not None
            gcp.maybe_update_md5(self._conf, path, st.version, result)
            return result
        elif _is_azure_path(path):
            st = azure.maybe_stat(self._conf, path)
            if st is None:
                raise FileNotFoundError(f"No such file: '{path}'")
            # https://docs.microsoft.com/en-us/rest/api/storageservices/get-blob-properties
            h = st.md5
            if h is None:
                # md5 is missing, calculate it and store it on file if the file has not changed
                with self.BlobFile(path, "rb") as f:
                    h = common.block_md5(f).hex()
                assert st.version is not None
                azure.maybe_update_md5(self._conf, path, st.version, h)
            return h
        else:
            with self.BlobFile(path, "rb") as f:
                return common.block_md5(f).hex()

    @overload
    def BlobFile(
        self,
        path: str,
        mode: Literal["rb", "wb", "ab"],
        streaming: Optional[bool] = ...,
        buffer_size: int = ...,
        cache_dir: Optional[str] = ...,
        file_size: Optional[int] = None,
    ) -> BinaryIO:
        ...

    @overload
    def BlobFile(
        self,
        path: str,
        mode: Literal["r", "w", "a"] = ...,
        streaming: Optional[bool] = ...,
        buffer_size: int = ...,
        cache_dir: Optional[str] = ...,
        file_size: Optional[int] = None,
    ) -> TextIO:
        ...

    def BlobFile(
        self,
        path: str,
        mode: Literal["r", "rb", "w", "wb", "a", "ab"] = "r",
        streaming: Optional[bool] = None,
        buffer_size: Optional[int] = None,
        cache_dir: Optional[str] = None,
        file_size: Optional[int] = None,
    ):
        """
        Open a local or remote file for reading or writing

        Args:
            path local or remote path
            mode: one of "r", "rb", "w", "wb", "a", "ab" indicating the mode to open the file in
            streaming: the default for `streaming` is `True` when `mode` is in `"r", "rb"` and `False` when `mode` is in `"w", "wb", "a", "ab"`.
                * `streaming=True`:
                    * Reading is done without downloading the entire remote file.
                    * Writing is done to the remote file directly, but only in chunks of a few MB in size. `flush()` will not cause an early write.
                    * Appending is not implemented.
                * `streaming=False`:
                    * Reading is done by downloading the remote file to a local file during the constructor.
                    * Writing is done by uploading the file on `close()` or during destruction.
                    * Appending is done by downloading the file during construction and uploading on `close()` or during destruction.
            buffer_size: number of bytes to buffer, this can potentially make reading more efficient.
            cache_dir: a directory in which to cache files for reading, only valid if `streaming=False` and `mode` is in `"r", "rb"`.   You are reponsible for cleaning up the cache directory.
            file_size: size of the file being opened, can be specified directly to avoid checking the file size when opening the file.  While this will avoid a network request, it also means that you may get an error when first reading a file that does not exist rather than when opening it.  Only valid for modes "r" and "rb".  This valid will be ignored for local files.

        Returns:
            A file-like object
        """
        if _guess_isdir(path):
            raise IsADirectoryError(f"Is a directory: '{path}'")

        if streaming is None:
            streaming = mode in ("r", "rb")

        if file_size is not None:
            assert mode in ("r", "rb"), "Can only specify file_size when reading"

        if _is_local_path(path) and "w" in mode:
            # local filesystems require that intermediate directories exist, but this is not required by the
            # remote filesystems
            # for consistency, automatically create local intermediate directories
            if self.dirname(path) != "":
                self.makedirs(self.dirname(path))

        if buffer_size is None:
            buffer_size = self._conf.default_buffer_size

        if streaming:
            if mode not in ("w", "wb", "r", "rb"):
                raise Error(f"Invalid mode for streaming file: '{mode}'")
            if cache_dir is not None:
                raise Error("Cannot specify cache_dir for streaming files")
            if _is_local_path(path):
                f = io.FileIO(path, mode=mode)
                if "r" in mode:
                    f = io.BufferedReader(f, buffer_size=buffer_size)
                else:
                    f = io.BufferedWriter(f, buffer_size=buffer_size)
            elif _is_gcp_path(path):
                if mode in ("w", "wb"):
                    f = gcp.StreamingWriteFile(self._conf, path)
                elif mode in ("r", "rb"):
                    f = gcp.StreamingReadFile(self._conf, path, size=file_size)
                    f = io.BufferedReader(f, buffer_size=buffer_size)
                else:
                    raise Error(f"Unsupported mode: '{mode}'")
            elif _is_azure_path(path):
                if mode in ("w", "wb"):
                    f = azure.StreamingWriteFile(self._conf, path)
                elif mode in ("r", "rb"):
                    f = azure.StreamingReadFile(self._conf, path, size=file_size)
                    f = io.BufferedReader(f, buffer_size=buffer_size)
                else:
                    raise Error(f"Unsupported mode: '{mode}'")
            else:
                raise Error(f"Unrecognized path: '{path}'")

            # this should be a protocol so we don't have to cast
            # but the standard library does not seem to have a file-like protocol
            binary_f = cast(BinaryIO, f)
            if "b" in mode:
                return binary_f
            else:
                text_f = io.TextIOWrapper(binary_f, encoding="utf8")
                # TextIOWrapper bypasses buffering on purpose: https://bugs.python.org/issue13393
                # Example: https://gist.github.com/christopher-hesse/b4aab4f6f9bcba597d079f3363dfab2c
                #
                # This happens when TextIOWrapper calls f.read1(CHUNK_SIZE)
                # https://github.com/python/cpython/blob/3d17c045b4c3d09b72bbd95ed78af1ae6f0d98d2/Modules/_io/textio.c#L1854
                # and BufferedReader only reads the requested size, not the buffer_size
                # https://github.com/python/cpython/blob/8666356280084f0426c28a981341f72eaaacd006/Modules/_io/bufferedio.c#L945
                #
                # The workaround appears to be to set the _CHUNK_SIZE property or monkey patch binary_f.read1 to call binary_f.read
                if hasattr(text_f, "_CHUNK_SIZE"):
                    setattr(text_f, "_CHUNK_SIZE", buffer_size)
                return cast(TextIO, text_f)
        else:
            remote_path = None
            tmp_dir = None
            if mode not in ("w", "wb", "r", "rb", "a", "ab"):
                raise Error(f"Invalid mode: '{mode}'")

            if cache_dir is not None and mode not in ("r", "rb"):
                raise Error("cache_dir only supported in read mode")

            local_filename = self.basename(path)
            if local_filename == "":
                local_filename = "local.tmp"
            if _is_gcp_path(path) or _is_azure_path(path):
                remote_path = path
                if mode in ("a", "ab"):
                    tmp_dir = tempfile.mkdtemp()
                    local_path = self.join(tmp_dir, local_filename)
                    if self.exists(remote_path):
                        self.copy(remote_path, local_path)
                elif mode in ("r", "rb"):
                    if cache_dir is None:
                        tmp_dir = tempfile.mkdtemp()
                        local_path = self.join(tmp_dir, local_filename)
                        self.copy(remote_path, local_path)
                    else:
                        if not _is_local_path(cache_dir):
                            raise Error(
                                f"cache_dir must be a local path: '{cache_dir}'"
                            )
                        self.makedirs(cache_dir)
                        path_md5 = hashlib.md5(path.encode("utf8")).hexdigest()
                        lock_path = self.join(cache_dir, f"{path_md5}.lock")
                        tmp_path = self.join(cache_dir, f"{path_md5}.tmp")
                        with filelock.FileLock(lock_path):
                            remote_version = ""
                            # get some sort of consistent remote hash so we can check for a local file
                            if _is_gcp_path(path):
                                st = gcp.maybe_stat(self._conf, path)
                                if st is None:
                                    raise FileNotFoundError(f"No such file: '{path}'")
                                assert st.version is not None
                                remote_version = st.version
                                remote_hash = st.md5
                            elif _is_azure_path(path):
                                # in the azure case the remote md5 may not exist
                                # this duplicates some of md5() because we want more control
                                st = azure.maybe_stat(self._conf, path)
                                if st is None:
                                    raise FileNotFoundError(f"No such file: '{path}'")
                                assert st.version is not None
                                remote_version = st.version
                                remote_hash = st.md5
                            else:
                                raise Error(f"Unrecognized path: '{path}'")

                            perform_copy = False
                            if remote_hash is None:
                                # there is no remote md5, copy the file
                                # and attempt to update the md5
                                perform_copy = True
                            else:
                                expected_local_path = self.join(
                                    cache_dir, remote_hash, local_filename
                                )
                                perform_copy = not self.exists(expected_local_path)

                            if perform_copy:
                                local_hexdigest = self.copy(
                                    remote_path,
                                    tmp_path,
                                    overwrite=True,
                                    return_md5=True,
                                )
                                assert (
                                    local_hexdigest is not None
                                ), "failed to return md5"
                                # the file we downloaded may not match the remote file because
                                # the remote file changed while we were downloading it
                                # in this case make sure we don't cache it under the wrong md5
                                local_path = self.join(
                                    cache_dir, local_hexdigest, local_filename
                                )
                                os.makedirs(self.dirname(local_path), exist_ok=True)
                                if os.path.exists(local_path):
                                    # the file is already here, nevermind
                                    os.remove(tmp_path)
                                else:
                                    os.replace(tmp_path, local_path)

                                if remote_hash is None:
                                    if _is_azure_path(path):
                                        azure.maybe_update_md5(
                                            self._conf,
                                            path,
                                            remote_version,
                                            local_hexdigest,
                                        )
                                    elif _is_gcp_path(path):
                                        gcp.maybe_update_md5(
                                            self._conf,
                                            path,
                                            remote_version,
                                            local_hexdigest,
                                        )
                            else:
                                assert remote_hash is not None
                                local_path = self.join(
                                    cache_dir, remote_hash, local_filename
                                )
                else:
                    tmp_dir = tempfile.mkdtemp()
                    local_path = self.join(tmp_dir, local_filename)
            elif _is_local_path(path):
                local_path = path
            else:
                raise Error(f"Unrecognized path: '{path}'")

            f = _ProxyFile(
                ctx=self,
                local_path=local_path,
                mode=mode,
                tmp_dir=tmp_dir,
                remote_path=remote_path,
            )
            if "r" in mode:
                f = io.BufferedReader(f, buffer_size=buffer_size)
            else:
                f = io.BufferedWriter(f, buffer_size=buffer_size)
            binary_f = cast(BinaryIO, f)
            if "b" in mode:
                return binary_f
            else:
                text_f = io.TextIOWrapper(binary_f, encoding="utf8")
                return cast(TextIO, text_f)


def default_log_fn(msg: str) -> None:
    print(f"blobfile: {msg}")


def _is_gcp_path(path: str) -> bool:
    url = urllib.parse.urlparse(path)
    return url.scheme == "gs"


def _is_azure_path(path: str) -> bool:
    url = urllib.parse.urlparse(path)
    return (
        url.scheme == "https" and url.netloc.endswith(".blob.core.windows.net")
    ) or url.scheme == "az"


def _get_module(path: str) -> Optional[ModuleType]:
    if _is_gcp_path(path):
        return gcp
    elif _is_azure_path(path):
        return azure
    else:
        return None


def _is_local_path(path: str) -> bool:
    return _get_module(path) is None


def _download_chunk(
    conf: Config, src: str, dst: str, start: int, size: int, src_file_size: int
) -> None:
    ctx = Context(conf)
    with ctx.BlobFile(src, "rb", file_size=src_file_size) as src_f:
        src_f.seek(start)
        # open output file such that we can write directly to the correct range
        with open(dst, "rb+") as dst_f:
            dst_f.seek(start)
            bytes_read = 0
            while True:
                n = min(CHUNK_SIZE, size - bytes_read)
                assert n >= 0
                block = src_f.read(n)
                if block == b"":
                    if bytes_read != size:
                        raise Error(
                            f"read wrong number of bytes from file `{src}`, expected {size} but read {bytes_read}"
                        )
                    break
                dst_f.write(block)
                bytes_read += len(block)


def _parallel_download(
    conf: Config,
    executor: concurrent.futures.Executor,
    src: str,
    dst: str,
    return_md5: bool,
) -> Optional[str]:
    ctx = Context(conf=conf)

    s = ctx.stat(src)

    # pre-allocate output file
    if os.path.dirname(dst) != "":
        os.makedirs(os.path.dirname(dst), exist_ok=True)
    with open(dst, "wb") as f:
        if s.size > 0:
            f.seek(s.size - 1)
            f.write(b"\0")

    max_workers = getattr(executor, "_max_workers", os.cpu_count() or 1)
    part_size = max(
        math.ceil(s.size / max_workers), common.PARALLEL_COPY_MINIMUM_PART_SIZE
    )
    start = 0
    futures = []
    while start < s.size:
        future = executor.submit(
            _download_chunk,
            conf,
            src,
            dst,
            start,
            min(part_size, s.size - start),
            s.size,
        )
        futures.append(future)
        start += part_size
    for future in futures:
        future.result()

    if return_md5:
        with ctx.BlobFile(dst, "rb") as f:
            return binascii.hexlify(common.block_md5(f)).decode("utf8")


def _string_overlap(s1: str, s2: str) -> int:
    length = min(len(s1), len(s2))
    for i in range(length):
        if s1[i] != s2[i]:
            return i
    return length


def _split_path(path: str) -> List[str]:
    # a/b/c => a/, b/, c
    # a/b/ => a/, b/
    # /a/b/c => /, a/, b/, c
    parts = []
    part = ""
    for c in path:
        part += c
        if c == "/":
            parts.append(part)
            part = ""
    if part != "":
        parts.append(part)
    return parts


def _expand_implicit_dirs(root: str, it: Iterator[DirEntry]) -> Iterator[DirEntry]:
    # blob storage does not always have definitions for each intermediate dir
    # if we have a listing like
    #  gs://test/a/b
    #  gs://test/a/b/c/d
    # then we emit an entry "gs://test/a/b/c" for the implicit dir "c"
    # requires that iterator return objects in sorted order
    if _is_gcp_path(root):
        entry_from_dirpath = gcp.entry_from_dirpath
    elif _is_azure_path(root):
        entry_from_dirpath = azure.entry_from_dirpath
    else:
        raise Error(f"Unrecognized path '{root}'")

    previous_path = root
    for entry in it:
        # find the overlap between the previous_path and the current
        entry_slash_path = _get_slash_path(entry)
        offset = _string_overlap(previous_path, entry_slash_path)
        relpath = entry_slash_path[offset:]
        cur = entry_slash_path[:offset]
        for part in _split_path(relpath)[:-1]:
            cur += part
            yield entry_from_dirpath(cur)
        yield entry
        assert entry_slash_path >= previous_path
        previous_path = entry_slash_path


def _compile_pattern(s: str, sep: str = "/"):
    tokens = [t for t in re.split("([*]+)", s) if t != ""]
    regexp = ""
    for tok in tokens:
        if tok == "*":
            regexp += f"[^{sep}]*"
        elif tok == "**":
            regexp += ".*"
        else:
            regexp += re.escape(tok)
    return re.compile(regexp + f"{sep}?$")


def _glob_full(conf: Config, pattern: str) -> Iterator[DirEntry]:
    prefix, _, _ = pattern.partition("*")

    re_pattern = _compile_pattern(pattern)

    for entry in _expand_implicit_dirs(
        root=prefix, it=_list_blobs(conf=conf, path=prefix)
    ):
        entry_slash_path = _get_slash_path(entry)
        if bool(re_pattern.match(entry_slash_path)):
            if entry_slash_path == prefix and entry.is_dir:
                # we matched the parent directory
                continue
            yield entry


class _GlobTask(NamedTuple):
    cur: str
    rem: Sequence[str]


class _GlobEntry(NamedTuple):
    entry: DirEntry


class _GlobTaskComplete(NamedTuple):
    pass


def _process_glob_task(
    conf: Config, root: str, t: _GlobTask
) -> Iterator[Union[_GlobTask, _GlobEntry]]:
    cur = t.cur + t.rem[0]
    rem = t.rem[1:]
    if "**" in cur:
        for entry in _glob_full(conf, root + cur + "".join(rem)):
            yield _GlobEntry(entry)
    elif "*" in cur:
        re_pattern = _compile_pattern(root + cur)
        prefix, _, _ = cur.partition("*")
        path = root + prefix
        for entry in _list_blobs(conf=conf, path=path, delimiter="/"):
            entry_slash_path = _get_slash_path(entry)
            # in the case of dirname/* we should not return the path dirname/
            if entry_slash_path == path and entry.is_dir:
                # we matched the parent directory
                continue
            if bool(re_pattern.match(entry_slash_path)):
                if len(rem) == 0:
                    yield _GlobEntry(entry)
                else:
                    assert entry_slash_path.startswith(root)
                    yield _GlobTask(entry_slash_path[len(root) :], rem)
    else:
        if len(rem) == 0:
            path = root + cur
            entry = _get_entry(conf, path)
            if entry is not None:
                yield _GlobEntry(entry)
        else:
            yield _GlobTask(cur, rem)


def _glob_worker(
    conf: Config,
    root: str,
    tasks: mp.Queue[_GlobTask],
    results: mp.Queue[Union[_GlobEntry, _GlobTask, _GlobTaskComplete]],
) -> None:
    while True:
        t = tasks.get()
        for r in _process_glob_task(conf, root=root, t=t):
            results.put(r)
        results.put(_GlobTaskComplete())


def _local_glob(pattern: str) -> Iterator[str]:
    normalized_pattern = os.path.normpath(pattern)
    if pattern.endswith("/") or pattern.endswith("\\"):
        # normpath will remove a trailing separator
        # but these affect the output of the glob
        normalized_pattern += os.sep

    if "*" in normalized_pattern:
        prefix = normalized_pattern.split("*")[0]
        if prefix.endswith(os.sep):
            base_dir = os.path.abspath(prefix)
            if not base_dir.endswith(os.sep):
                # if base_dir is the root directory it will already have a separator
                base_dir += os.sep
            pattern_suffix = normalized_pattern[len(prefix) :]
        else:
            dirpath = os.path.dirname(prefix)
            base_dir = os.path.abspath(dirpath)
            # prefix   dirpath  pattern_suffix
            #   /a/b   /a       /b
            #   a/c    a        /c
            #   b      ""       ""
            #   ""     ""       ""
            if len(dirpath) == 0:
                pattern_suffix = os.sep + normalized_pattern
            else:
                pattern_suffix = normalized_pattern[len(dirpath) :]
        full_pattern = base_dir + pattern_suffix
        regexp = _compile_pattern(full_pattern, sep=os.sep)
        for root, dirnames, filenames in os.walk(base_dir):
            paths = [os.path.join(root, dirname + os.sep) for dirname in dirnames]
            paths += [os.path.join(root, filename) for filename in filenames]
            for path in paths:
                if re.match(regexp, path):
                    if path.endswith(os.sep):
                        path = path[:-1]
                    yield path
    else:
        path = os.path.abspath(pattern)
        if os.path.exists(path):
            if path.endswith(os.sep):
                path = path[:-1]
            yield path


def _strip_slash(path: str) -> str:
    if path.endswith("/"):
        return path[:-1]
    else:
        return path


def _guess_isdir(path: str) -> bool:
    """
    Guess if a path is a directory without performing network requests
    """
    if _is_local_path(path) and os.path.isdir(path):
        return True
    elif (_is_gcp_path(path) or _is_azure_path(path)) and path.endswith("/"):
        return True
    return False


def _list_blobs(
    conf: Config, path: str, delimiter: Optional[str] = None
) -> Iterator[DirEntry]:
    params = {}
    if delimiter is not None:
        params["delimiter"] = delimiter

    if _is_gcp_path(path):
        yield from gcp.list_blobs(conf, path, delimiter=delimiter)
    elif _is_azure_path(path):
        yield from azure.list_blobs(conf, path, delimiter=delimiter)
    else:
        raise Error(f"Unrecognized path: '{path}'")


def _get_slash_path(entry: DirEntry) -> str:
    return entry.path + "/" if entry.is_dir else entry.path


def _normalize_path(conf: Config, path: str) -> str:
    # convert paths to the canonical format
    if _is_azure_path(path):
        return azure.combine_path(conf, *azure.split_path(path))
    return path


def _list_blobs_in_dir(
    conf: Config, prefix: str, exclude_prefix: bool
) -> Iterator[DirEntry]:
    # the prefix check doesn't work without normalization
    normalized_prefix = _normalize_path(conf, prefix)
    for entry in _list_blobs(conf=conf, path=normalized_prefix, delimiter="/"):
        if exclude_prefix and _get_slash_path(entry) == normalized_prefix:
            continue
        yield entry


def _get_entry(conf: Config, path: str) -> Optional[DirEntry]:
    ctx = Context(conf)
    if _is_gcp_path(path):
        st = gcp.maybe_stat(conf, path)
        if st is not None:
            if path.endswith("/"):
                return gcp.entry_from_dirpath(path)
            else:
                return gcp.entry_from_path_stat(path, st)
        if ctx.isdir(path):
            return gcp.entry_from_dirpath(path)
    elif _is_azure_path(path):
        st = azure.maybe_stat(conf, path)
        if st is not None:
            if path.endswith("/"):
                return azure.entry_from_dirpath(path)
            else:
                return azure.entry_from_path_stat(path, st)
        if ctx.isdir(path):
            return azure.entry_from_dirpath(path)
    else:
        raise Error(f"Unrecognized path: '{path}'")

    return None


def _sharded_listdir_worker(
    conf: Config,
    prefixes: mp.Queue[Tuple[str, str, bool]],
    items: mp.Queue[Optional[DirEntry]],
) -> None:
    while True:
        base, prefix, exact = prefixes.get(True)
        if exact:
            path = base + prefix
            entry = _get_entry(conf, path)
            if entry is not None:
                items.put(entry)
        else:
            it = _list_blobs_in_dir(conf, base + prefix, exclude_prefix=False)
            for entry in it:
                items.put(entry)
        items.put(None)  # indicate that we have finished this path


def _join2(conf: Config, a: str, b: str) -> str:
    if _is_local_path(a):
        return os.path.join(a, b)
    elif _is_gcp_path(a):
        return gcp.join_paths(conf, a, b)
    elif _is_azure_path(a):
        return azure.join_paths(conf, a, b)
    else:
        raise Error(f"Unrecognized path: '{a}'")


class _ProxyFile(io.FileIO):
    def __init__(
        self,
        ctx: Context,
        local_path: str,
        mode: 'Literal["r", "rb", "w", "wb", "a", "ab"]',
        tmp_dir: Optional[str],
        remote_path: Optional[str],
    ) -> None:
        super().__init__(local_path, mode=mode)
        self._ctx = ctx
        self._mode = mode
        self._tmp_dir = tmp_dir
        self._local_path = local_path
        self._remote_path = remote_path
        self._closed = False

    def close(self) -> None:
        if not hasattr(self, "_closed") or self._closed:
            return

        super().close()
        try:
            if self._remote_path is not None and self._mode in ("w", "wb", "a", "ab"):
                self._ctx.copy(self._local_path, self._remote_path, overwrite=True)
        finally:
            # if the copy fails, still cleanup our local temp file so it is not leaked
            if self._tmp_dir is not None:
                os.remove(self._local_path)
                os.rmdir(self._tmp_dir)
        self._closed = True


def create_context(
    *,
    log_callback: Callable[[str], None] = default_log_fn,
    connection_pool_max_size: int = DEFAULT_CONNECTION_POOL_MAX_SIZE,
    max_connection_pool_count: int = DEFAULT_MAX_CONNECTION_POOL_COUNT,
    # https://docs.microsoft.com/en-us/rest/api/storageservices/understanding-block-blobs--append-blobs--and-page-blobs#about-block-blobs
    # the chunk size determines the maximum size of an individual blob
    azure_write_chunk_size: int = DEFAULT_AZURE_WRITE_CHUNK_SIZE,
    google_write_chunk_size: int = DEFAULT_GOOGLE_WRITE_CHUNK_SIZE,
    retry_log_threshold: int = DEFAULT_RETRY_LOG_THRESHOLD,
    retry_common_log_threshold: int = DEFAULT_RETRY_COMMON_LOG_THRESHOLD,
    retry_limit: Optional[int] = None,
    connect_timeout: Optional[int] = DEFAULT_CONNECT_TIMEOUT,
    read_timeout: Optional[int] = DEFAULT_READ_TIMEOUT,
    output_az_paths: bool = True,
    use_azure_storage_account_key_fallback: bool = False,
    get_http_pool: Optional[Callable[[], urllib3.PoolManager]] = None,
    use_streaming_read: bool = False,
    default_buffer_size: int = DEFAULT_BUFFER_SIZE,
    get_deadline: Optional[Callable[[], float]] = None,
    save_access_token_to_disk: bool = True,
    multiprocessing_start_method: str = "spawn",
):
    """
    Same argument as configure(), but returns a Context object that has all the blobfile methods on it.
    """
    conf = Config(
        log_callback=log_callback,
        connection_pool_max_size=connection_pool_max_size,
        max_connection_pool_count=max_connection_pool_count,
        azure_write_chunk_size=azure_write_chunk_size,
        retry_log_threshold=retry_log_threshold,
        retry_common_log_threshold=retry_common_log_threshold,
        retry_limit=retry_limit,
        google_write_chunk_size=google_write_chunk_size,
        connect_timeout=connect_timeout,
        read_timeout=read_timeout,
        output_az_paths=output_az_paths,
        use_azure_storage_account_key_fallback=use_azure_storage_account_key_fallback,
        get_http_pool=get_http_pool,
        use_streaming_read=use_streaming_read,
        default_buffer_size=default_buffer_size,
        get_deadline=get_deadline,
        save_access_token_to_disk=save_access_token_to_disk,
        multiprocessing_start_method=multiprocessing_start_method,
    )
    return Context(conf=conf)
